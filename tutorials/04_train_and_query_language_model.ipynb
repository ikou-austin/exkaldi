{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Exkaldi\n",
    "\n",
    "In this section, we will train a n-grams language model and query it.\n",
    "\n",
    "Althrough __Srilm__ is avaliable in exkaldi, we recommend __Kenlm__ toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exkaldi\n",
    "\n",
    "import os\n",
    "dataDir = os.path.join(\"..\",\"examplesdata\",\"librispeech_dummy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly,  prepare the lexicons.\n",
    "\n",
    "We have generated and saved a LexiconBank object in file already (3_prepare_lexicons). So restorage it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<exkaldi.decode.graph.LexiconBank at 0x7f79cc9f8a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexFile = os.path.join(dataDir, \"exp\", \"lexicons.lex\")\n",
    "\n",
    "lexicons = exkaldi.decode.graph.load_lex(lexFile)\n",
    "\n",
    "lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use __Srilm__ backend, run this code to prepare it firstly. If __Kenlm__ backend, you don't need to do anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/misc/home/usr18/wangyu/.virtualenvs/tfenv/bin:/home/usr18/wangyu/anaconda3/bin:/usr/local/cuda/bin:/home/usr18/wangyu/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/snap/bin:/Work18/wangyu/kaldi/src/Gambian:/Work18/wangyu/kaldi/src/bin:/Work18/wangyu/kaldi/tools/openfst:/Work18/wangyu/kaldi/tools/openfst/bin:/Work18/wangyu/kaldi/src/featbin:/Work18/wangyu/kaldi/src/GAMbian:/Work18/wangyu/kaldi/src/nnetbin:/Work18/wangyu/kaldi/src/lmbin:/Work18/wangyu/kaldi/src/fstbin:/Work18/wangyu/kaldi/src/latbin:/Work18/wangyu/kaldi/src/gmmbin:/Work18/wangyu/kaldi/tools/srilm:/Work18/wangyu/kaldi/tools/srilm/bin:/Work18/wangyu/kaldi/tools/srilm/bin/i686-m64'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExkaldiInfo = exkaldi.version\n",
    "\n",
    "ExkaldiInfo.prepare_srilm()\n",
    "\n",
    "ExkaldiInfo.ENV[\"PATH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have prepared a transcription file in the data directory, we do not need the utterance-ID information at the head of each line. Because we don't have extra corpus avaliable now, we must take a bit of work to produce one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = os.path.join(dataDir, \"train\", \"text\")\n",
    "newTextFile = os.path.join(dataDir, \"exp\", \"text_no_uttID\")\n",
    "\n",
    "exkaldi.utils.make_dependent_dirs(newTextFile, True)\n",
    "\n",
    "with open(textFile, \"r\", encoding=\"utf-8\") as fr:\n",
    "    lines = fr.readlines()\n",
    "\n",
    "temp = []\n",
    "for line in lines:\n",
    "    line = line.strip().split(maxsplit=1)\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    else:\n",
    "        temp.append(line[1])\n",
    "\n",
    "with open(newTextFile, \"w\", encoding=\"utf-8\") as fw:\n",
    "    fw.write( \"\\n\".join(temp) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train a 3-grams model with __Kenlm__ backend. (In exkaldi version 1.0, Error will be raised if RAM is not enough.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/misc/Work19/wangyu/exkaldi-1.0/examplesdata/librispeech_dummy/exp/3-gram.arpa'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arpaFile = os.path.join(dataDir, \"exp\", \"3-gram.arpa\")\n",
    "\n",
    "exkaldi.lm.train_ngrams_kenlm(lexicons, order=3, textFile=newTextFile, outFile=arpaFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARPA model can be transform to binary format in order to accelerate loading and reduce memory cost.\n",
    "\n",
    "Although KenLm python API supports reading ARPA format, but in exkaldi, we only expected KenLM Binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/misc/Work19/wangyu/exkaldi-1.0/examplesdata/librispeech_dummy/exp/3-gram.binary'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binaryLmFile = os.path.join(dataDir, \"exp\", \"3-gram.binary\")\n",
    "\n",
    "exkaldi.lm.arpa_to_binary(arpaFile, binaryLmFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the binary LM file to initialize a Python KenLM n-grams object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = exkaldi.lm.KenNGrams(binaryLmFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__KenNGrams__ is but a simple wrapper of KenLM python Model. N-grams up to 6 orders can be use.\n",
    "\n",
    "Query it with a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.523445129394531"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(\"HELLO WORLD\", bos=True, eos=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
