{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Exkaldi\n",
    "\n",
    "In this section, we will train a n-grams language model and query it.\n",
    "\n",
    "Althrough __Srilm__ is avaliable in exkaldi, we recommend __Kenlm__ toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exkaldi\n",
    "\n",
    "import os\n",
    "dataDir = \"librispeech_dummy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, prepare the lexicons. We have generated and saved a __LexiconBank__ object in file already (3_prepare_lexicons). So restorage it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<exkaldi.decode.graph.LexiconBank at 0x7f2175144278>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexFile = os.path.join(dataDir, \"exp\", \"lexicons.lex\")\n",
    "\n",
    "lexicons = exkaldi.decode.graph.load_lex(lexFile)\n",
    "\n",
    "lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use training text corpus to train LM model. Even though we have prepared a transcription file in the data directory, we do not need the utterance-ID information at the head of each line, so we must take a bit of work to produce a new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = os.path.join(dataDir, \"train\", \"text\")\n",
    "newTextFile = os.path.join(dataDir, \"exp\", \"train_lm_text\")\n",
    "\n",
    "exkaldi.utils.make_dependent_dirs(newTextFile, True)\n",
    "\n",
    "with open(textFile, \"r\", encoding=\"utf-8\") as fr:\n",
    "    lines = fr.readlines()\n",
    "\n",
    "temp = []\n",
    "for line in lines:\n",
    "    line = line.strip().split(maxsplit=1)\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    else:\n",
    "        temp.append(line[1])\n",
    "\n",
    "with open(newTextFile, \"w\", encoding=\"utf-8\") as fw:\n",
    "    fw.write( \"\\n\".join(temp) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train a 2-grams model with __Kenlm__ backend. (__srilm__ backend is also avaliable.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/misc/Work19/wangyu/exkaldi-1.2/tutorials/librispeech_dummy/exp/2-gram.arpa'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arpaFile = os.path.join(dataDir, \"exp\", \"2-gram.arpa\")\n",
    "\n",
    "exkaldi.lm.train_ngrams_kenlm(lexicons, order=2, textFile=newTextFile, outFile=arpaFile, config={\"-S\":\"20%\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARPA model can be transform to binary format in order to accelerate loading and reduce memory cost.  \n",
    "Although __KenLm__ Python API supports reading ARPA format, but in exkaldi, we only expected KenLM Binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/misc/Work19/wangyu/exkaldi-1.2/tutorials/librispeech_dummy/exp/2-gram.binary'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binaryLmFile = os.path.join(dataDir, \"exp\", \"2-gram.binary\")\n",
    "\n",
    "exkaldi.lm.arpa_to_binary(arpaFile, binaryLmFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the binary LM file to initialize a Python KenLM n-grams object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<exkaldi.lm.lm.KenNGrams at 0x7f2174ece160>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = exkaldi.lm.KenNGrams(binaryLmFile)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__KenNGrams__ is simple wrapper of KenLM python Model. Defaultly, N-grams up to 6 orders can be use. If you want to use training bigger N-Grams, change the configure when you install the exkaldi pypi package from our github.\n",
    "\n",
    "You can query this model with a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.531333923339844"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(\"HELLO WORLD\", bos=True, eos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or compute the perplexity to evaluate a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1272-128104-0000': 389.82728639223217,\n",
       " '1272-128104-0001': 1259.8703186980435,\n",
       " '1272-128104-0002': 634.7298958519973,\n",
       " '1272-128104-0003': 700.4998070862594,\n",
       " '1272-128104-0004': 640.942524338446,\n",
       " '1272-135031-0000': 884.1503819354043,\n",
       " '1272-135031-0001': 398.2710010651754,\n",
       " '1272-135031-0002': 237.274884147695,\n",
       " '1272-135031-0003': 246.7711624456322,\n",
       " '1272-135031-0004': 445.0821986932373,\n",
       " '1272-141231-0000': 242.7351101272028,\n",
       " '1272-141231-0001': 632.2589965581269,\n",
       " '1272-141231-0002': 563.2843089015221,\n",
       " '1272-141231-0003': 533.2710688117827,\n",
       " '1272-141231-0004': 370.62070021926473,\n",
       " '1462-170138-0000': 467.84169223100076,\n",
       " '1462-170138-0001': 598.6470090870347,\n",
       " '1462-170138-0002': 722.879667569615,\n",
       " '1462-170138-0003': 1245.2999525714333,\n",
       " '1462-170138-0004': 116.89921221929579}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalTrans = exkaldi.load_trans(os.path.join(dataDir, \"test\", \"text\"))\n",
    "\n",
    "score = model.perplexity(evalTrans)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exkaldi.core.achivements.Metric"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___score___ is an exkaldi __Metric__ (a subclass of Python dict) object. We design a group of classes to hold Kaldi text format table and exkaldi own text format data.\n",
    "\n",
    "__ListTable__: spk2utt, utt2spk, words, phones and so on.  \n",
    "__Transcription__: transcription corpus, n-best decoding result and so on.  \n",
    "__Metric__: AM score, LM score, LM perplexity, Sentence lengthes and so on.  \n",
    "__BytesIndexTable__: the memory index of a bytes data.  \n",
    "\n",
    "All these classes are subclasses of Python dict. They have some same and respective methods and attributes. For example, we can compute the average value of __Metric__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566.5578589475201"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate the Grammar fst for futher steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/misc/Work19/wangyu/exkaldi-1.2/tutorials/librispeech_dummy/exp/G.fst'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gfile = os.path.join(dataDir, \"exp\", \"G.fst\")\n",
    "\n",
    "exkaldi.decode.graph.make_G(lexicons, arpaFile, outFile=Gfile, order=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
