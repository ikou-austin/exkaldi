{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Exkaldi\n",
    "\n",
    "In this section, we will training a DNN acoustic model with Tensorflow 2.x.\n",
    "\n",
    "If you want run this step, please install tensorflow firstly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exkaldi\n",
    "\n",
    "import os\n",
    "dataDir = \"librispeech_dummy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use keras to build and train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n",
    "\n",
    "Restorage the training feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featFile = os.path.join(dataDir, \"exp\", \"mfcc.ark\")\n",
    "\n",
    "feat = exkaldi.load_feat(featFile)\n",
    "\n",
    "feat = feat.to_numpy()\n",
    "\n",
    "feat.dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature is made following these steps:\n",
    "\n",
    "compute mfcc (13) >> apply CMVN (13) >> add 2 order deltas (39) >> splice 1-1 frames (117)\n",
    "\n",
    "We still further do global standerd normalization on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = feat.normalize(std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Them we load the alignment data. They have been generated in early step (07_train_triphone_HMM-GMM_delta).\n",
    "\n",
    "We will use pdf-ID as target label. In exkaldi, transition-ID and phone-ID can also be extracted for mutiple tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<exkaldi.core.achivements.NumpyAlignmentPdf at 0x7f48487043c8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aliFile = os.path.join(dataDir, \"exp\", \"train_delta\", \"final.ali\")\n",
    "hmmFile = os.path.join(dataDir, \"exp\", \"train_delta\", \"final.mdl\")\n",
    "\n",
    "ali = exkaldi.load_ali(aliFile)\n",
    "\n",
    "ali = ali.to_numpy(aliType=\"pdfID\", hmm=hmmFile)\n",
    "\n",
    "ali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look the classes of alignment. It is the output units of Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we tuple the feature and alignment in order to generate a dataset for Neural Network Framework. We use __tuple_data(...)__ function to group them. \n",
    "\n",
    "But note that, this function will group the achivements by their name, so please ensure their names are avaliable as python identifiers. (that means, we only allow lower and upper letters, digits, and underline in their names.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126345"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.rename(\"mfcc\")\n",
    "ali.rename(\"pdfID\")\n",
    "\n",
    "dataset = exkaldi.tuple_data([feat,ali], frameLevel=True)\n",
    "\n",
    "datasetSize = len(dataset)\n",
    "datasetSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___dataset___ is a list. whose members are namedtuples. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TupledData(uttID='103-1240-0000', frameID=0, mfcc=array([[-8.19768488e-01, -1.98504940e-01,  6.30387783e-01,\n",
       "         9.09560174e-02,  8.71950507e-01,  1.24062955e+00,\n",
       "         1.16157448e+00,  3.28367263e-01,  4.53087598e-01,\n",
       "         6.50599599e-02,  2.01714620e-01,  6.11037135e-01,\n",
       "         3.36491138e-01,  6.70793874e-04,  2.54764557e-02,\n",
       "         1.77971616e-01,  1.78947791e-01,  4.53171022e-02,\n",
       "        -5.15161633e-01, -4.16369766e-01,  2.91420579e-01,\n",
       "         6.82660222e-01,  5.25227606e-01, -3.15611064e-01,\n",
       "        -2.79526472e-01,  1.53618842e-01,  2.50820909e-02,\n",
       "         3.96822095e-02,  2.34689027e-01, -2.32635170e-01,\n",
       "         3.47851105e-02, -6.70426860e-02, -1.50040478e-01,\n",
       "        -1.28142163e-01,  4.15323287e-01,  6.23874485e-01,\n",
       "        -1.92458287e-01, -1.82956874e-01, -5.52129328e-01,\n",
       "        -8.19626927e-01, -1.98620737e-01,  6.30370140e-01,\n",
       "         9.10242200e-02,  8.71933877e-01,  1.24058509e+00,\n",
       "         1.16157663e+00,  3.28481555e-01,  4.53085810e-01,\n",
       "         6.49296939e-02,  2.01607585e-01,  6.11081004e-01,\n",
       "         3.36555958e-01,  7.04614737e-04,  2.54822318e-02,\n",
       "         1.78008422e-01,  1.78886399e-01,  4.52871807e-02,\n",
       "        -5.15170932e-01, -4.16330934e-01,  2.91462421e-01,\n",
       "         6.82643414e-01,  5.25196195e-01, -3.15589309e-01,\n",
       "        -2.79536247e-01,  1.53639778e-01,  2.50826273e-02,\n",
       "         3.97229232e-02,  2.34646574e-01, -2.32604519e-01,\n",
       "         3.47633474e-02, -6.69955388e-02, -1.50005445e-01,\n",
       "        -1.28236428e-01,  4.15337056e-01,  6.23914778e-01,\n",
       "        -1.92494363e-01, -1.82972834e-01, -5.52141130e-01,\n",
       "        -8.25532019e-01, -2.19186753e-01,  5.94998717e-01,\n",
       "         2.56303400e-01,  1.03900886e+00,  1.07305944e+00,\n",
       "         1.11492956e+00,  9.31876421e-01,  7.96952307e-01,\n",
       "         2.41211504e-01, -1.59678549e-01,  2.17802405e-01,\n",
       "         6.72561347e-01,  1.38742961e-02,  1.68287773e-02,\n",
       "         4.88060892e-01,  1.83874965e-01,  1.79936051e-01,\n",
       "        -5.21256506e-01, -4.30746228e-01,  3.03994358e-01,\n",
       "         8.75858963e-01,  8.35931718e-01, -5.50294101e-01,\n",
       "        -5.49693644e-01, -8.94124389e-01,  1.27125755e-02,\n",
       "        -3.63075435e-02, -6.37455806e-02, -5.79648077e-01,\n",
       "         2.64800992e-02,  3.71973217e-01,  1.26982793e-01,\n",
       "        -3.41435581e-01, -1.78238407e-01,  2.27382198e-01,\n",
       "         1.51581049e-01,  4.36486602e-01, -4.73617107e-01]], dtype=float32), pdfID=array([0], dtype=int32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneRecord = dataset[0]\n",
    "\n",
    "oneRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use name to get specified data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneRecord.pdfID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you train a sequential NN model, you may not want to tuple achivemnts data in framelevel but in utterance level. try to change the mode of tuple. \n",
    "\n",
    "You can tuple all kinds of exkaldi achivements such as feature, CMVN, alignment, probability. And even different feature such as MFCC, fBank and so on, different alignment such as PdfID, Phone ID. For example, now we want to do multiple tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ali2 = exkaldi.load_ali(aliFile)\n",
    "\n",
    "ali2 = ali2.to_numpy(aliType=\"phoneID\", hmm=hmmFile)\n",
    "\n",
    "ali2.rename(\"phoneID\")\n",
    "\n",
    "dataset2 = exkaldi.tuple_data([feat,ali,ali2], frameLevel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TupledData(uttID='103-1240-0000', frameID=0, mfcc=array([[-8.19768488e-01, -1.98504940e-01,  6.30387783e-01,\n",
       "         9.09560174e-02,  8.71950507e-01,  1.24062955e+00,\n",
       "         1.16157448e+00,  3.28367263e-01,  4.53087598e-01,\n",
       "         6.50599599e-02,  2.01714620e-01,  6.11037135e-01,\n",
       "         3.36491138e-01,  6.70793874e-04,  2.54764557e-02,\n",
       "         1.77971616e-01,  1.78947791e-01,  4.53171022e-02,\n",
       "        -5.15161633e-01, -4.16369766e-01,  2.91420579e-01,\n",
       "         6.82660222e-01,  5.25227606e-01, -3.15611064e-01,\n",
       "        -2.79526472e-01,  1.53618842e-01,  2.50820909e-02,\n",
       "         3.96822095e-02,  2.34689027e-01, -2.32635170e-01,\n",
       "         3.47851105e-02, -6.70426860e-02, -1.50040478e-01,\n",
       "        -1.28142163e-01,  4.15323287e-01,  6.23874485e-01,\n",
       "        -1.92458287e-01, -1.82956874e-01, -5.52129328e-01,\n",
       "        -8.19626927e-01, -1.98620737e-01,  6.30370140e-01,\n",
       "         9.10242200e-02,  8.71933877e-01,  1.24058509e+00,\n",
       "         1.16157663e+00,  3.28481555e-01,  4.53085810e-01,\n",
       "         6.49296939e-02,  2.01607585e-01,  6.11081004e-01,\n",
       "         3.36555958e-01,  7.04614737e-04,  2.54822318e-02,\n",
       "         1.78008422e-01,  1.78886399e-01,  4.52871807e-02,\n",
       "        -5.15170932e-01, -4.16330934e-01,  2.91462421e-01,\n",
       "         6.82643414e-01,  5.25196195e-01, -3.15589309e-01,\n",
       "        -2.79536247e-01,  1.53639778e-01,  2.50826273e-02,\n",
       "         3.97229232e-02,  2.34646574e-01, -2.32604519e-01,\n",
       "         3.47633474e-02, -6.69955388e-02, -1.50005445e-01,\n",
       "        -1.28236428e-01,  4.15337056e-01,  6.23914778e-01,\n",
       "        -1.92494363e-01, -1.82972834e-01, -5.52141130e-01,\n",
       "        -8.25532019e-01, -2.19186753e-01,  5.94998717e-01,\n",
       "         2.56303400e-01,  1.03900886e+00,  1.07305944e+00,\n",
       "         1.11492956e+00,  9.31876421e-01,  7.96952307e-01,\n",
       "         2.41211504e-01, -1.59678549e-01,  2.17802405e-01,\n",
       "         6.72561347e-01,  1.38742961e-02,  1.68287773e-02,\n",
       "         4.88060892e-01,  1.83874965e-01,  1.79936051e-01,\n",
       "        -5.21256506e-01, -4.30746228e-01,  3.03994358e-01,\n",
       "         8.75858963e-01,  8.35931718e-01, -5.50294101e-01,\n",
       "        -5.49693644e-01, -8.94124389e-01,  1.27125755e-02,\n",
       "        -3.63075435e-02, -6.37455806e-02, -5.79648077e-01,\n",
       "         2.64800992e-02,  3.71973217e-01,  1.26982793e-01,\n",
       "        -3.41435581e-01, -1.78238407e-01,  2.27382198e-01,\n",
       "         1.51581049e-01,  4.36486602e-01, -4.73617107e-01]], dtype=float32), pdfID=array([0], dtype=int32), phoneID=array([2], dtype=int32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDim = feat.dim\n",
    "pdfClasses = exkaldi.hmm.load_hmm(hmmFile,\"triphone\").info.pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ali2\n",
    "del dataset2\n",
    "\n",
    "del ali\n",
    "del feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now we start to train DNN acoustic model. Fisrtly, design a data iterator from our provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generater(dataset, batchSize):\n",
    "\n",
    "    length = len(dataset)\n",
    "    while True:\n",
    "        index = 0\n",
    "        random.shuffle(dataset)\n",
    "        while index < length:\n",
    "            one = dataset[index]\n",
    "            index += 1\n",
    "            yield (one.mfcc[0], one.pdfID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 64\n",
    "tf_datasets = tf.data.Dataset.from_generator(\n",
    "                                 lambda : data_generater(dataset),\n",
    "                                 (tf.float32, tf.int32)\n",
    "                            ).batch(batchSize).prefetch(batchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define a simple Dense model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_DNN_model(inputsShape, classes):\n",
    "    \n",
    "    inputs = keras.Input(inputsShape)\n",
    "    h1 = keras.layers.Dense(256, activation=\"relu\", kernel_initializer=\"he_normal\")(inputs)\n",
    "    h1_bn = keras.layers.BatchNormalization()(h1)\n",
    "    \n",
    "    h2 = keras.layers.Dense(512, activation=\"relu\", kernel_initializer=\"he_normal\")(h1_bn)\n",
    "    h2_bn = keras.layers.BatchNormalization()(h2)\n",
    "    \n",
    "    h3 = keras.layers.Dense(512, activation=\"relu\", kernel_initializer=\"he_normal\")(h2_bn)\n",
    "    h3_bn = keras.layers.BatchNormalization()(h3)\n",
    "    \n",
    "    outputs = keras.layers.Dense(classes, use_bias=False)(h3_bn)\n",
    "    \n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 117)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               30208     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 784)               401408    \n",
      "=================================================================\n",
      "Total params: 830,976\n",
      "Trainable params: 828,416\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_DNN_model((featureDim,), pdfClasses)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are optimizer and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(0.001)\n",
    "\n",
    "losses = keras.metrics.Mean(name=\"train/loss\", dtype=tf.float32)\n",
    "accs = keras.metrics.Mean(name=\"train/accuracy\", dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speecify the output dir. You can use tensorboard to check the training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDir = os.path.join(dataDir, \"exp\", \"train_DNN\")\n",
    "\n",
    "stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logDir = os.path.join(outDir, \"log\", stamp)\n",
    "file_writer = tf.summary.create_file_writer(logDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1974"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "epoch_iterations = datasetSize//batchSize\n",
    "\n",
    "epoch_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to print the progress bar and control the epoch ending, we will lend a hand from __tqdm__ package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /misc/home/usr18/wangyu/.virtualenvs/tfenv/lib/python3.6/site-packages (4.43.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install tqdm 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to train this model. During the training loop, You can use tensorboard to look the visiable training result.\n",
    "\n",
    "```\n",
    "tensorboard --logdir=./librispeech_dummy/exp/train_DNN/log --bind_all\n",
    "```\n",
    "\n",
    "Just for fun, we do not validate the model during the training, but in real case, you should do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1974/1974 [00:44<00:00, 44.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Loss 2.581684  Acc 0.393665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:44<00:00, 44.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss 2.141212  Acc 0.467456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:44<00:00, 44.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  Loss 1.881780  Acc 0.516480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:44<00:00, 44.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3  Loss 1.693251  Acc 0.555180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:44<00:00, 44.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4  Loss 1.545394  Acc 0.586849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:44<00:00, 44.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5  Loss 1.424021  Acc 0.614029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:44<00:00, 44.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6  Loss 1.322298  Acc 0.637538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:43<00:00, 45.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7  Loss 1.235971  Acc 0.657878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:44<00:00, 44.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8  Loss 1.161078  Acc 0.675820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:43<00:00, 45.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9  Loss 1.096026  Acc 0.691521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:44<00:00, 44.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10  Loss 1.038808  Acc 0.705696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:44<00:00, 44.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11  Loss 0.988151  Acc 0.718406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:45<00:00, 43.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12  Loss 0.942649  Acc 0.729912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:44<00:00, 44.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13  Loss 0.902020  Acc 0.740264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:43<00:00, 44.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14  Loss 0.864805  Acc 0.749970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:43<00:00, 45.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15  Loss 0.831406  Acc 0.758669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:44<00:00, 44.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16  Loss 0.800684  Acc 0.766786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:44<00:00, 44.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17  Loss 0.772631  Acc 0.774187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:44<00:00, 43.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18  Loss 0.746672  Acc 0.781133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1974/1974 [00:44<00:00, 44.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19  Loss 0.723037  Acc 0.787453\n",
      "Training Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with file_writer.as_default():\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for batch,i in zip(tf_datasets, tqdm(range(epoch_iterations))):\n",
    "            data, label = batch\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(data, training=True)\n",
    "                loss = keras.losses.sparse_categorical_crossentropy(label, logits, from_logits=True)\n",
    "                losses(loss)\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "                pred = keras.backend.argmax(logits, axis=1)\n",
    "\n",
    "                acc = exkaldi.nn.accuracy(label.numpy(), pred.numpy())\n",
    "                accs(acc.accuracy)\n",
    "        \n",
    "            #if int(optimizer.iterations.numpy()) % epoch_iterations == 0:     #<<<< if you don't use tqdm\n",
    "            #    break\n",
    "        \n",
    "        current_loss = losses.result()\n",
    "        current_acc = accs.result()\n",
    "        tf.print( f\"Epoch {epoch}\", f\" Loss {current_loss:.6f}\", f\" Acc {current_acc:.6f}\")\n",
    "\n",
    "        tf.summary.scalar(\"train/loss\", data=current_loss, step=epoch)\n",
    "        tf.summary.scalar(\"train/accuracy\", data=current_acc, step=epoch)\n",
    "\n",
    "    tf.print( \"Training Done\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model in file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfModelFile = os.path.join(outDir, \"dnn.h5\")\n",
    "\n",
    "model.save(tfModelFile, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we predict the network output for test data for decoding. We do the same processing as training feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeatFile = os.path.join(dataDir, \"exp\", \"test_mfcc.ark\")\n",
    "\n",
    "testFeat = exkaldi.load_feat(testFeatFile)\n",
    "\n",
    "testFeat = testFeat.to_numpy()\n",
    "\n",
    "testFeat = testFeat.normalize(std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<exkaldi.core.achivements.NumpyProbability at 0x7f481fb70f98>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = {}\n",
    "for utt, mat in testFeat.items:\n",
    "    logits = model(mat, training=False)\n",
    "    prob[utt] = logits.numpy()\n",
    "\n",
    "prob = exkaldi.load_prob(prob)\n",
    "\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___prob___ is an exkaldi __NumpyProbability__ object. Save it to file. We will decode it in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'librispeech_dummy/exp/train_DNN/amp.npy'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probFile = os.path.join(outDir, \"amp.npy\")\n",
    "\n",
    "prob.save(probFile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
