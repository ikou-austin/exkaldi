{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Exkaldi\n",
    "\n",
    "In this section, we will training a DNN acoustic model with Tensorflow 2.x.\n",
    "\n",
    "If you want run this step, please install tensorflow firstly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exkaldi\n",
    "\n",
    "import os\n",
    "dataDir = os.path.join(\"..\",\"examplesdata\",\"librispeech_dummy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use keras to build and train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n",
    "\n",
    "Restorage the training feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<exkaldi.core.achivements.NumpyFeature at 0x7f2543d59f28>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featFile = os.path.join(dataDir,\"exp\",\"mfcc.ark\")\n",
    "\n",
    "feat = exkaldi.load_feat(featFile)\n",
    "\n",
    "feat = feat.to_numpy()\n",
    "\n",
    "feat.dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature is made following these steps:\n",
    "\n",
    "compute mfcc (13) >> apply CMVN (13) >> add 2 order deltas (39) >> splice 1-1 frames (117)\n",
    "\n",
    "We still further do global standerd normalization on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = feat.normalize(std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Them we load the alignment data. They have been generated in early step (07_train_triphone_HMM-GMM_delta).\n",
    "\n",
    "We will use pdf-ID as target label. In exkaldi, transition-ID and phone-ID can also be extracted for mutiple tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aliFile = os.path.join(dataDir, \"exp\", \"train_delta\", \"final.ali\")\n",
    "hmmFile = os.path.join(dataDir, \"exp\", \"train_delta\", \"final.mdl\")\n",
    "\n",
    "ali = exkaldi.load_ali(aliFile)\n",
    "\n",
    "ali = ali.to_numpy(aliType=\"pdfID\", hmm=hmmFile)\n",
    "\n",
    "ali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look the classes of alignment. It is the output units of Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ali.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we tuple the feature and alignment in order to generate a dataset for Neural Network Framework. We use __tuple_data(...)__ function to group them. \n",
    "\n",
    "But note that, this function will group the achivements by their name, so please ensure their names are avaliable as python identifiers. (that means, we only allow lower and upper letters, digits, and underline in their names.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat.rename(\"mfcc\")\n",
    "ali.rename(\"pdfID\")\n",
    "\n",
    "dataset = exkaldi.tuple_data([feat,ali], frameLevel=True)\n",
    "\n",
    "datasetSize = len(dataset)\n",
    "datasetSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___dataset___ is a list. whose members are namedtuples. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-55a9d71005c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "oneRecord = dataset[0]\n",
    "\n",
    "oneRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use name to get specified data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneRecord.pdfID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you train a sequential NN model, you may not want to tuple achivemnts data in framelevel but in utterance level. try to change the mode of tuple. \n",
    "\n",
    "You can tuple all kinds of exkaldi achivements such as feature, CMVN, alignment, probability. And even different feature such as MFCC, fBank and so on, different alignment such as PdfID, Phone ID. For example, now we want to do multiple tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ali2 = exkaldi.load_ali(aliFile)\n",
    "\n",
    "ali2 = ali2.to_numpy(aliType=\"phoneID\", hmm=hmmFile)\n",
    "\n",
    "ali2.rename(\"phoneID\")\n",
    "\n",
    "dataset2 = exkaldi.tuple_data([feat,ali,ali], frameLevel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ali2\n",
    "del dataset2\n",
    "\n",
    "del ali\n",
    "del feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now we start to train DNN acoustic model. Fisrtly, design a data iterator from our provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generater(dataset):\n",
    "    length = len(dataset)\n",
    "    while True:\n",
    "        index = 0\n",
    "        random.shuffle(dataset)\n",
    "        while index < length:\n",
    "            one = dataset[index]\n",
    "            index += 1\n",
    "            yield (one.mfcc[0], one.pdfID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 64\n",
    "tf_datasets = tf.data.Dataset.from_generator(\n",
    "                                 lambda : data_generater(dataset),\n",
    "                                 (tf.float32, tf.int32)\n",
    "                            ).batch(batchSize).prefetch(batchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define a simple Dense model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_DNN_model(inputsShape, classes):\n",
    "    \n",
    "    inputs = keras.Input(inputsShape)\n",
    "    h1 = keras.layers.Dense(256, activation=\"relu\", kernel_initializer=\"he_normal\")(inputs)\n",
    "    h1_bn = keras.layers.BatchNormalization()(h1)\n",
    "    \n",
    "    h2 = keras.layers.Dense(512, activation=\"relu\", kernel_initializer=\"he_normal\")(h1_bn)\n",
    "    h2_bn = keras.layers.BatchNormalization()(h2)\n",
    "    \n",
    "    h3 = keras.layers.Dense(512, activation=\"relu\", kernel_initializer=\"he_normal\")(h2_bn)\n",
    "    h3_bn = keras.layers.BatchNormalization()(h3)\n",
    "    \n",
    "    outputs = keras.layers.Dense(classes, use_bias=False)(h3_bn)\n",
    "    \n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_DNN_model((feat.dim,), ali.classes)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are optimizer and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(0.001)\n",
    "\n",
    "losses = keras.metrics.Mean(name=\"train/loss\", dtype=tf.float32)\n",
    "accs = keras.metrics.Mean(name=\"train/accuracy\", dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speecify the output dir. You can use tensorboard to check the training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDir = os.path.join(dataDir, \"exp\", \"train_DNN\")\n",
    "\n",
    "stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logDir = os.path.join(outDir, \"log\", stamp)\n",
    "file_writer = tf.summary.create_file_writer(logDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "epoch_iterations = datasetSize//batchSize\n",
    "maxmum_iterations = epochs * epoch_iterations\n",
    "\n",
    "epoch_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with file_writer.as_default():\n",
    "    \n",
    "    for batch in tf_datasets:\n",
    "        data, label = batch\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            logits = model(data, training=True)\n",
    "            loss = keras.losses.sparse_categorical_crossentropy(label, logits, from_logits=True)\n",
    "            losses(loss)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "            pred = keras.backend.argmax(logits, axis=1)\n",
    "            \n",
    "            acc = exkaldi.nn.accuracy(label.numpy(), pred.numpy())\n",
    "            accs(acc.accuracy)\n",
    "        \n",
    "        train_step = optimizer.iterations.numpy()\n",
    "        if train_step % epoch_iterations == 0 or train_step > maxmum_iterations:\n",
    "            current_loss = losses.result()\n",
    "            current_acc = accs.result()\n",
    "            tf.print( f\"Step {train_step}/{maxmum_iterations}\", f\" Loss {current_loss:.6f}\", f\" Acc {current_acc:.6f}\")\n",
    "            \n",
    "            tf.summary.scalar(\"train/loss\", data=current_loss, step=train_step)\n",
    "            tf.summary.scalar(\"train/accuracy\", data=current_acc, step=train_step)\n",
    "        \n",
    "            if train_step > maxmum_iterations:\n",
    "                tf.print( \"Training Done\" )\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model in file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfModelFile = os.path.join(outDir, \"dnn.h5\")\n",
    "\n",
    "model.save(tfModelFile, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we predict the network output for test data for decoding. We do the same processing as training feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeatFile = os.path.join(dataDir, \"exp\", \"test_mfcc.ark\")\n",
    "\n",
    "testFeat = exkaldi.load_feat(testFeatFile)\n",
    "\n",
    "testFeat = testFeat.to_numpy()\n",
    "\n",
    "testFeat = testFeat.normalize(std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = {}\n",
    "for utt, mat in testFeat.items:\n",
    "    logits = model(mat, training=False)\n",
    "    prob[utt] = logits.numpy()\n",
    "\n",
    "prob = exkaldi.load_prob(prob)\n",
    "\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___prob___ is an exkaldi __NumpyProbability__ object. Save it to file. We will decode it in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../examplesdata/librispeech_light/exp/train_DNN/prob.npy'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probFile = os.path.join(outDir, \"amp.npy\")\n",
    "\n",
    "prob.save(probFile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
